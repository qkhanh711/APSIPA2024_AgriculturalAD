{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenVINO is not installed. Please install OpenVINO to use OpenVINOInferencer.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import anomalib\n",
    "from anomalib.data import PredictDataset, MVTec\n",
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Fastflow, EfficientAd, Patchcore, Ganomaly, Draem, Padim\n",
    "from anomalib.utils.post_processing import superimpose_anomaly_map\n",
    "from anomalib import TaskType\n",
    "\n",
    "root = \"../../../../patchcore-inspection/mvtec_anomaly_detection\"\n",
    "cls = \"bean\"\n",
    "task = \"segmentation\"\n",
    "model_name = \"pathcore\"\n",
    "batch_size = 8\n",
    "num_workers = 8\n",
    "img_size = 128\n",
    "test_id = \"0000\" if cls == \"bean\" else \"000\"\n",
    "\n",
    "\n",
    "if task.upper() == \"SEGMENTATION\":\n",
    "    task = TaskType.SEGMENTATION\n",
    "elif task.upper() == \"CLASSIFICATION\":\n",
    "    task = TaskType.CLASSIFICATION\n",
    "elif task.upper() == \"DETECTION\":\n",
    "    task = TaskType.DETECTION\n",
    "else:\n",
    "    raise ValueError(\"Task type not supported\")\n",
    "\n",
    "\n",
    "datamodule = MVTec(\n",
    "    root= root,\n",
    "    category= cls,\n",
    "    train_batch_size= batch_size,\n",
    "    eval_batch_size= batch_size,\n",
    "    num_workers= num_workers,\n",
    "    task=task,\n",
    "    image_size=(img_size, img_size),\n",
    ")\n",
    "\n",
    "\n",
    "model =  Patchcore(\n",
    "    coreset_sampling_ratio= 0.01\n",
    ")\n",
    "\n",
    "model\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        mode=\"max\",\n",
    "        monitor=\"pixel_AUROC\",\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"pixel_AUROC\",\n",
    "        mode=\"max\",\n",
    "        patience=3,\n",
    "    ),\n",
    "]\n",
    "\n",
    "engine = Engine(\n",
    "    callbacks=callbacks,\n",
    "    pixel_metrics=\"AUROC\",\n",
    "    accelerator=\"gpu\",  # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "    devices=1,\n",
    "    logger=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/khanhnq/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model                 │ PatchcoreModel           │ 24.9 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _transform            │ Compose                  │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ normalization_metrics │ MinMax                   │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ image_threshold       │ F1AdaptiveThreshold      │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ pixel_threshold       │ F1AdaptiveThreshold      │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ image_metrics         │ AnomalibMetricCollection │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ pixel_metrics         │ AnomalibMetricCollection │      0 │ train │\n",
       "└───┴───────────────────────┴──────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model                 │ PatchcoreModel           │ 24.9 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _transform            │ Compose                  │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ normalization_metrics │ MinMax                   │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ image_threshold       │ F1AdaptiveThreshold      │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ pixel_threshold       │ F1AdaptiveThreshold      │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ image_metrics         │ AnomalibMetricCollection │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ pixel_metrics         │ AnomalibMetricCollection │      0 │ train │\n",
       "└───┴───────────────────────┴──────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 24.9 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 24.9 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 99                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 24.9 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 24.9 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 99                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9288cee4062947f4b5453f5e09aaa577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/khanhnq/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automat\n",
       "ic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/khanhnq/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automat\n",
       "ic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43295803bacb4b33b3e02cf53ef76041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a44c97bf3d64a849d5250211f0ebce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9445269703865051     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8458617925643921     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        pixel_AUROC        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9660123586654663     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9445269703865051    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8458617925643921    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       pixel_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9660123586654663    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Engine' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m engine\u001b[38;5;241m.\u001b[39mfit(datamodule\u001b[38;5;241m=\u001b[39mdatamodule, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m      3\u001b[0m engine\u001b[38;5;241m.\u001b[39mtest(datamodule\u001b[38;5;241m=\u001b[39mdatamodule, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m----> 6\u001b[0m image_AUROC, image_F1Score, pixel_AUROC \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m(datamodule\u001b[38;5;241m=\u001b[39mdatamodule, model\u001b[38;5;241m=\u001b[39mmodel)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Engine' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "engine.fit(datamodule=datamodule, model=model)\n",
    "\n",
    "engine.test(datamodule=datamodule, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4850f9f97a14969922dc2a91a14bcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9445269703865051     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8458617925643921     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        pixel_AUROC        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9660123586654663     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9445269703865051    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8458617925643921    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       pixel_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9660123586654663    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = engine.test(datamodule=datamodule, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_AUROC': 0.9660123586654663, 'image_AUROC': 0.9445269703865051, 'image_F1Score': 0.8458617925643921}\n"
     ]
    }
   ],
   "source": [
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../patchcore-inspection/mvtec_anomaly_detection/bean/test/tear/0000.png\n"
     ]
    }
   ],
   "source": [
    "print(root + f\"/{cls}/test/tear/{test_id}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_dataset = PredictDataset(path=root + f\"/{cls}/test/tear/{test_id}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpt_path is not provided. Model weights will not be loaded.\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bfc9f3397d4247a30da02f905c3717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "inference_dataloader = DataLoader(dataset=inference_dataset, num_workers=47)\n",
    "\n",
    "predictions = engine.predict(model=model, dataloaders=inference_dataloader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: torch.Size([1, 3, 112, 112]),\n",
      "Anomaly Map Shape: torch.Size([1, 1, 112, 112]), \n",
      "Predicted Mask Shape: torch.Size([1, 1, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\n",
    "    f'Image Shape: {predictions[\"image\"].shape},\\n'\n",
    "    f'Anomaly Map Shape: {predictions[\"anomaly_maps\"].shape}, \\n'\n",
    "    f'Predicted Mask Shape: {predictions[\"pred_masks\"].shape}',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 112, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_path = predictions[\"image_path\"][0]\n",
    "image_size = predictions[\"image\"].shape[-2:]\n",
    "image = np.array(Image.open(image_path).resize(image_size))\n",
    "print(image.shape)\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/khanhnq/Code/Research/computer_vision/EvaluateAD/APSIPA2024_AgriculturalAD/trainningAD/script\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5913) tensor(True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc48c209c30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.savefig(f\"{cls}_image.png\")\n",
    "plt.show()\n",
    "import cv2\n",
    "cv2.imwrite(f\"{cls}_predict.png\", image)\n",
    "\n",
    "anomaly_map = predictions[\"anomaly_maps\"][0]\n",
    "anomaly_map = anomaly_map.cpu().numpy().squeeze()\n",
    "plt.savefig(f\"{cls}_anomaly_map.png\")\n",
    "plt.imshow(anomaly_map)\n",
    "# plt.show()\n",
    "\n",
    "heat_map = superimpose_anomaly_map(anomaly_map=anomaly_map, image=image, normalize=True)\n",
    "plt.savefig(f\"{cls}_heat_map.png\")\n",
    "plt.imshow(heat_map)\n",
    "# plt.show()\n",
    "\n",
    "pred_score = predictions[\"pred_scores\"][0]\n",
    "pred_labels = predictions[\"pred_labels\"][0]\n",
    "print(pred_score, pred_labels)\n",
    "\n",
    "pred_masks = predictions[\"pred_masks\"][0].squeeze().cpu().numpy()\n",
    "plt.savefig(f\"{cls}_pred_masks.png\")\n",
    "plt.imshow(pred_masks)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
